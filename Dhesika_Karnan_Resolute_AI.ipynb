{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYUEJpgufC9kWDaPJgLZ8O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhesika23/ML-StreamLit/blob/main/Dhesika_Karnan_Resolute_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "BiVsq3pan2Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOvJ-6O6I_Nl",
        "outputId": "a7de9ba0-ad7c-4a6c-8977-5a04e2b681d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "from scipy.spatial import distance\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data for Task 1 (Clustering)\n",
        "train_data_path = \"/content/drive/MyDrive/train.xlsx\"\n",
        "train_df = pd.read_excel(train_data_path)\n",
        "\n",
        "# Load train data for Task 2 (Classification)\n",
        "train_path_classification = \"/content/drive/MyDrive/train.xlsx\"\n",
        "train_df_classification = pd.read_excel(train_path_classification)\n",
        "\n",
        "# Load test data for Task 2 (Classification)\n",
        "test_path_classification = \"/content/drive/MyDrive/test.xlsx\"\n",
        "test_df_classification = pd.read_excel(test_path_classification)\n",
        "\n",
        "# Load raw data for Task 3\n",
        "raw_data_path_task3 = \"/content/drive/MyDrive/rawdata.xlsx\"\n",
        "raw_data_task3 = pd.read_excel(raw_data_path_task3)\n",
        "\n",
        "# Handle missing values for clustering\n",
        "def handle_missing_values(df):\n",
        "    columns_with_missing = df.columns[df.isnull().any()]\n",
        "    for col in columns_with_missing:\n",
        "        if df[col].dtype == 'object':\n",
        "            imputer = SimpleImputer(strategy='most_frequent')\n",
        "            df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
        "        else:\n",
        "            imputer = SimpleImputer(strategy='mean')\n",
        "            df[col] = imputer.fit_transform(df[[col]]).ravel()\n",
        "    return df\n",
        "\n",
        "# Handle outliers using winsorization for clustering\n",
        "def handle_outliers(df):\n",
        "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    for col in numeric_columns:\n",
        "        df[col] = winsorize(df[col], limits=[0.05, 0.05])\n",
        "    return df\n",
        "\n",
        "# Apply preprocessing to training data for clustering\n",
        "train_df_cleaned = handle_missing_values(train_df)\n",
        "train_df_cleaned = handle_outliers(train_df_cleaned)\n",
        "\n",
        "# Removing the target data as it is not necessary for clustering\n",
        "X_train_cluster = train_df_cleaned.drop('target', axis=1)\n",
        "\n",
        "# Standardize features for clustering\n",
        "scaler_cluster = StandardScaler()\n",
        "X_train_scaled_cluster = scaler_cluster.fit_transform(X_train_cluster)\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_train_scaled_cluster)\n",
        "\n",
        "# Calculate silhouette score for clustering\n",
        "silhouette_avg = silhouette_score(X_train_scaled_cluster, cluster_labels)\n",
        "\n",
        "# Add cluster labels to the training DataFrame for clustering using .loc to avoid SettingWithCopyWarning\n",
        "train_df.loc[:, 'cluster'] = cluster_labels\n",
        "\n",
        "# Identify the cluster and Mahalanobis distance for a given data point for clustering\n",
        "def identify_cluster(data_point):\n",
        "    cluster = kmeans.predict([data_point])[0]\n",
        "    centroid = kmeans.cluster_centers_[cluster]\n",
        "    mahalanobis_dist = distance.mahalanobis(data_point, centroid, np.linalg.inv(np.cov(X_train_scaled_cluster.T)))\n",
        "    return cluster, mahalanobis_dist\n",
        "\n",
        "# Load train data for classification\n",
        "X_train_classification = train_df_classification.drop(['target'], axis=1)\n",
        "y_train_classification = train_df_classification['target']\n",
        "\n",
        "# Fill missing values with mean for classification\n",
        "X_train_classification.fillna(X_train_classification.mean(), inplace=True)\n",
        "\n",
        "# Standardize features for classification\n",
        "scaler_classification = StandardScaler()\n",
        "X_train_scaled_classification = scaler_classification.fit_transform(X_train_classification)\n",
        "\n",
        "# Initialize the model for classification\n",
        "clf_classification = RandomForestClassifier()\n",
        "\n",
        "# Train the model for classification\n",
        "clf_classification.fit(X_train_classification, y_train_classification)\n",
        "\n",
        "# Make predictions on the train data for classification\n",
        "y_pred_classification = clf_classification.predict(X_train_classification)\n",
        "\n",
        "test_predictions_classification = clf_classification.predict(test_df_classification)\n",
        "\n",
        "# Calculate train accuracy for classification\n",
        "train_accuracy_classification = accuracy_score(y_train_classification, y_pred_classification)\n",
        "\n",
        "# Create a DataFrame with row numbers and corresponding predicted targets for classification\n",
        "test_predictions_df_classification = pd.DataFrame({\n",
        "    'Row': test_df_classification.index + 1,\n",
        "    'Target': test_predictions_classification\n",
        "})\n",
        "\n",
        "# Clean and process raw data for Task 3\n",
        "def clean_and_process_data(raw_data):\n",
        "    raw_data['date'] = pd.to_datetime(raw_data['date'])\n",
        "    raw_data['date'] = raw_data['date'].dt.strftime('%d/%m/%Y')\n",
        "    raw_data['time'] = raw_data['time'].astype(str)\n",
        "    raw_data['datetime'] = pd.to_datetime(raw_data['date'] + ' ' + raw_data['time'])\n",
        "    raw_data['duration'] = raw_data.groupby('number')['datetime'].diff().fillna(pd.Timedelta(seconds=0))\n",
        "    return raw_data\n",
        "\n",
        "# Clean and process the data for Task 3\n",
        "processed_data_task3 = clean_and_process_data(raw_data_task3)\n",
        "\n",
        "# Aggregate the data by date for Task 3\n",
        "result_task3 = processed_data_task3.groupby('date').agg(\n",
        "    pick_activities=('activity', lambda x: (x == 'picked').sum()),\n",
        "    place_activities=('activity', lambda x: (x == 'placed').sum()),\n",
        "    inside_duration=('duration', lambda x: x[processed_data_task3['position'] == 'inside'].sum()),\n",
        "    outside_duration=('duration', lambda x: x[processed_data_task3['position'] == 'Outside'].sum())\n",
        ")\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Clustering, Classification and Aggregation\")\n",
        "\n",
        "    # Task 1: Clustering\n",
        "    st.header(\"Task 1: Machine Learning - Clustering\")\n",
        "    selected_data_point = st.sidebar.selectbox('Select a data point for clustering:', X_train_scaled_cluster.tolist())\n",
        "    cluster, mahalanobis_dist = identify_cluster(np.array(selected_data_point))\n",
        "    st.write(f'Selected Data Point: {selected_data_point}')\n",
        "    st.write(f'Predicted Cluster: {cluster}')\n",
        "    st.write(f'Mahalanobis Distance to Cluster Centroid: {mahalanobis_dist}')\n",
        "\n",
        "    # Task 2: Classification\n",
        "    st.header(\"Task 2: Machine Learning - Classification\")\n",
        "    st.write(f'Train Accuracy (Classification): {train_accuracy_classification}')\n",
        "    st.write(test_predictions_df_classification)\n",
        "\n",
        "    # Task 3: Data Aggregation\n",
        "    st.header(\"Task 3: Python - Aggregating Data by Date\")\n",
        "    st.header(\"Processed Data:\")\n",
        "    st.write(processed_data_task3)\n",
        "    st.header(\"Aggregated Result by Date:\")\n",
        "    st.write(result_task3)\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw26HrFmStHp",
        "outputId": "b310517a-7ddc-478f-c375-c22e00078bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny50joU1S17Q",
        "outputId": "6cbb4a3b-899e-4ec1-fa63-9b94cbc52139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 1.333s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7NZtD9JTGVk",
        "outputId": "2ac5e93e-046d-4581-8801-415b92703656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.16.166.115\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.989s\n",
            "your url is: https://neat-parrots-decide.loca.lt\n"
          ]
        }
      ]
    }
  ]
}